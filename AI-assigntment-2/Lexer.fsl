// The generated lexer module will start with this code
{
module Lexer
open FSharp.Text.Lexing
open System
// open the module that defines the tokens
open Parser
}

// We define macros for some regular expressions we will use later
let digit       = ['0'-'9']
let num         = digit+ ( '.' digit+)?  ('E' ('+'|'-')? digit+ )?
let whitespace  = [' ' '\t']
let newline     = "\n\r" | '\n' | '\r'
let var         = ['a'-'z''A'-'Z']['a'-'z''A'-'Z''_' ]*

// We define now the rules for recognising and building tokens
// for each of the tokens of our language we need a rule
// NOTE: rules are applied in order top-down. 
//       This is important when tokens overlap (not in this example)
rule tokenize = parse
// deal with tokens that need to be ignored (skip them)
| whitespace    { tokenize lexbuf }
| newline       { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }
// deal with tokens that need to be built
| num           { NUM(Double.Parse(LexBuffer<_>.LexemeString lexbuf)) }
| var           { VAR(LexBuffer<_>.LexemeString lexbuf) }
| '*'           { MULTIPLY }
| '/'           { DIVIDE }
| '+'           { ADDITION }
| '-'           { SUBTRACT }
| '0'           { ZERO }
| '^'           { POWER }
| ','           { COMMA }
| "true"        { TRUE }
| "false"       { FALSE }
| '&'           { SHORTAND }
| '|'           { SHORTOR }
| "&&"          { AND }
| "||"          { OR }
| '!'           { NEG }
| '='           { EQUAL }
| "!="          { NEGEQUAL }
| '>'           { GT }
| ">="          { GTE }
| '<'           { LT }
| "<="          { LTE }
| '('           { LPAR }
| ')'           { RPAR }
| '{'           { LCUR }
| '}'           { RCUR }
| '['           { LBRA }
| ']'           { RBRA }
| ":="          { ASSIGN }
| ';'           { SEQUENCE }
| "skip"        { SKIP }
| "if "         { IF }
| " fi"         { FI }
| "do "         { DO }
| " od"         { OD }
| "->"          { CONDITION }
| "[]"          { ELSE }
| eof           { EOF }